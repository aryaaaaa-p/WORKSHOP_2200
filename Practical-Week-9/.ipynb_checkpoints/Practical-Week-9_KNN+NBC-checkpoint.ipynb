{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Practical Tasks\n",
    "## KNN Classifier and Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classifying Iris Species with KNN Classifier\n",
    "In this section, we will go through a simple machine learning application and create\n",
    "our first classification model. In the process, we will introduce some core concepts and terms.\n",
    "\n",
    "Let’s assume that a hobby botanist is interested in distinguishing the species of some\n",
    "iris flowers that she has found. She has collected some measurements associated with\n",
    "each iris: the length and width of the petals and the length and width of the sepals, all\n",
    "measured in centimeters.\n",
    "\n",
    "She also has the measurements of some irises that have been previously identified by\n",
    "an expert botanist as belonging to the species setosa, versicolor, or virginica. For these\n",
    "measurements, she can be certain of which species each iris belongs to. Let’s assume\n",
    "that these are the only species our hobby botanist will encounter in the wild.\n",
    "\n",
    "Our goal is to build a machine learning model that can learn from the measurements\n",
    "of these irises whose species is known, so that we can predict the species for a new\n",
    "iris.\n",
    "\n",
    "Reference: Introduction to Machine learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Import Data\n",
    "\n",
    "The sklearn package provides some built-in real-world data sets to let users experience working on a real-world data analysis applications. The Iris data set is one of them. Please refer to https://scikit-learn.org/stable/datasets/index.html for more information about these built-in data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: \n",
      " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: \n",
      " ['setosa' 'versicolor' 'virginica']\n",
      "Feature data size: \n",
      " (150, 4)\n",
      "Target data size: \n",
      " (150,)\n",
      "Target values: \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "float_formatter = \"{:.6f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris_data = datasets.load_iris()\n",
    "\n",
    "print(\"Feature names: \\n\", iris_data.feature_names)\n",
    "print(\"Target names: \\n\", iris_data.target_names)\n",
    "\n",
    "print(\"Feature data size: \\n\", iris_data.data.shape)\n",
    "print(\"Target data size: \\n\", iris_data.target.shape)\n",
    "print(\"Target values: \\n\", iris_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Training and Testing Data\n",
    "\n",
    "We want to build a machine learning model from this data that can predict the species\n",
    "of iris for a new set of measurements. But before we can apply our model to new\n",
    "measurements, we need to know whether it actually works—that is, whether we\n",
    "should trust its predictions.\n",
    "\n",
    "To assess the model’s performance, we show it new data (data that it hasn’t seen\n",
    "before) for which we have labels. This is usually done by splitting the labeled data we\n",
    "have collected (here, our 150 flower measurements) into two parts. One part of the\n",
    "data is used to build our machine learning model, and is called the training data or\n",
    "training set. The rest of the data will be used to assess how well the model works; this\n",
    "is called the test data, test set, or hold-out set.\n",
    "\n",
    "scikit-learn contains a function that shuffles the dataset and splits it for you: the\n",
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) function. As default, this function extracts 75% of the rows in the data as the\n",
    "training set, together with the corresponding labels for this data. The remaining 25%\n",
    "of the data, together with the remaining labels, is declared as the test set. Deciding\n",
    "how much data you want to put into the training and the test set respectively is somewhat\n",
    "arbitrary, but using a test set containing 25% of the data is a good rule of thumb.\n",
    "\n",
    "In scikit-learn, data is usually denoted with a capital X, while labels are denoted by\n",
    "a lowercase y. This is inspired by the standard formulation f(x)=y in mathematics,\n",
    "where x is the input to a function and y is the output. Following more conventions\n",
    "from mathematics, we use a capital X because the data is a two-dimensional array (a\n",
    "matrix) and a lowercase y because the target is a one-dimensional array (a vector).\n",
    "Let’s call train_test_split on our data and assign the outputs using this nomenclature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making the split, the train_test_split function shuffles the dataset using a\n",
    "pseudorandom number generator. If we just took the last 25% of the data as a test set,\n",
    "all the data points would have the label 2, as the data points are sorted by the label\n",
    "(see the output for iris['target'] shown earlier). Using a test set containing only\n",
    "one of the three classes would not tell us much about how well our model generalizes. This will result in situation that the training data and testing data have different distributions.\n",
    "So, we shuffle our data to make sure the test data contains data from all classes.\n",
    "\n",
    "To make sure that we will get the same output if we run the same function several\n",
    "times, we provide the pseudorandom number generator with a fixed seed using the\n",
    "random_state parameter. This will make the outcome deterministic, so this line will\n",
    "always have the same outcome. We will always fix the random_state in this way when\n",
    "using randomized procedures.\n",
    "\n",
    "The output of the train_test_split function is X_train, X_test, y_train, and\n",
    "y_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,\n",
    "and X_test contains the remaining 25%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (112, 4)\n",
      "y_train shape: (112,)\n",
      "X_test shape: (38, 4)\n",
      "y_test shape: (38,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, random_state=142)\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### K-Nearest Neighbours Classifier\n",
    "\n",
    "Now we can start building the actual machine learning model. There are many classification\n",
    "algorithms in scikit-learn that we could use. Here we will use a k-nearest\n",
    "neighbors classifier, which is easy to understand. Building this model only consists of\n",
    "storing the training set. To make a prediction for a new data point, the algorithm\n",
    "finds the point in the training set that is closest to the new point. Then it assigns the\n",
    "label of this training point to the new data point.\n",
    "\n",
    "All machine learning models in scikit-learn are implemented in their own classes,\n",
    "which are called Estimator classes. The k-nearest neighbors classification algorithm\n",
    "is implemented in the [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) class in the neighbors module. Before\n",
    "we can use the model, we need to instantiate the class into an object. This is when we\n",
    "will set any parameters of the model. The most important parameter of KNeighbors\n",
    "Classifier is the number of neighbors (i.e., $K$), which we will set to 1 for our first exploration.\n",
    "\n",
    "**Model Training**: To build the model on the training set, we call the 'fit' method of the knn object,\n",
    "which takes as arguments the NumPy array X_train containing the training data and\n",
    "the NumPy array y_train of the corresponding training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Build a KNN classifier model\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Train the model with the training data\n",
    "clf_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**: We can now make predictions using this model on new data for which we might not\n",
    "know the correct labels. Imagine we found an iris in the wild with a sepal length of\n",
    "5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\n",
    "What species of iris would this be? We can put this data into a NumPy array, again by\n",
    "calculating the shape—that is, the number of samples (1) multiplied by the number of\n",
    "features (4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new.shape: (1, 4)\n",
      "The predicted class is: \n",
      " [0]\n"
     ]
    }
   ],
   "source": [
    "# Produce the features of a testing data instance\n",
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "print(\"X_new.shape: {}\".format(X_new.shape))\n",
    "\n",
    "# Predict the result label of X_new:\n",
    "y_new_pred = clf_knn.predict(X_new)\n",
    "print(\"The predicted class is: \\n\", y_new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicts that this new iris belongs to the class 0, meaning its species is setosa. But how do we know whether we can trust our model? We don’t know the correct species of this sample, which is the whole point of building the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating Model**: This is where the test set that we created earlier comes in. This data was not used to build the model, but we do know what the correct species is for each iris in the test set. So, we can use the trained model to predict these data instances and calculate the accuracy to evaluate how good the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "Write code to calculate the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.89474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [Your code here ...]\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf_knn.predict(X_test)\n",
    "\n",
    "accuracy  =  accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Accuracy is %.5f\\n\" %accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Parameter Tuning with Cross Validation (CV)\n",
    "\n",
    "In this section, we’ll explore a CV method that can be used to tune the hyperparameter $K$ using the above training and test data.\n",
    "\n",
    "Scikit-learn comes in handy with its [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) method. We specifiy that we are performing 10 folds with the cv=KFold(n_splits=10, shuffle=True) parameter and that our scoring metric should be accuracy since we are in a classification setting. In each iteration, the training data take 90% of the total data while testing data takes 10%. The average on the accuracies reported from each iteration will make the testing accuracy more robust than just a single split of the data.\n",
    "\n",
    "**Manual tuning with cross validation**: Plot the misclassification error versus $K$. You need to figure out the possible values of $K$. If the number of possible values is too big, you can take some values with a certain step, e.g., $K$ = 1, 5, 10, ... with a step of 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv_scores = []\n",
    "cv_scores_std = []\n",
    "\n",
    "# GridSearchCV: automatically search best perameter from hyper-parameters of a container\n",
    "# first GridSearch: in the predefined range, search the best perameter by the step in test data\n",
    "# second repeat GridSearch in cross validation\n",
    "# search neighbours from[1,6,11...134]\n",
    "k_range = range(1, 135, 5)\n",
    "for i in k_range:\n",
    "    #fidtd we select 1, second time we select 6, until 135\n",
    "    clf = KNeighborsClassifier(n_neighbors = i)\n",
    "    scores = cross_val_score(clf, iris_data.data, iris_data.target, scoring='accuracy', cv=KFold(n_splits=10, shuffle=True))\n",
    "    cv_scores.append(scores.mean())\n",
    "    cv_scores_std.append(scores.std())\n",
    "    \n",
    "# Plot the relationship\n",
    "# errorbar shows the error range, yerr is error range from lower to higher\n",
    "# x = k_range, y = cv_scores\n",
    "plt.errorbar(k_range, cv_scores, yerr=cv_scores_std, marker='x', label='Accuracy')\n",
    "# y axis range\n",
    "plt.ylim([0.1, 1.1])\n",
    "# draw x axis \n",
    "plt.xlabel('$K$')\n",
    "# draw y axis,\n",
    "plt.ylabel('Accuracy')\n",
    "# find best location for image label\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the accuracy first goes up when $K$ increases. It peeks around 15. Then, it keeps going down. Particularly, the performance (measured by the score mean) and its robustness/stableness (measured by the score std) drop substantially around K=85. One possible reason is that when K is bigger than 85, the model suffers from the underfitting issue severely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automated Parameter Tuning**: Use the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) method to accomplish automatic model selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Check against the figure plotted above to see if the selected hyperparameter $K$ can lead to the highest misclassification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value:  11\n",
      "The accuracy: 0.9667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameter_grid = {'n_neighbors': range(1, 135, 5)}\n",
    "knn_clf = KNeighborsClassifier()\n",
    "gs_knn = GridSearchCV(knn_clf, parameter_grid, scoring='accuracy', cv=KFold(n_splits=10, shuffle=True, random_state=1))\n",
    "gs_knn.fit(iris_data.data, iris_data.target)\n",
    "\n",
    "print('Best K value: ', gs_knn.best_params_['n_neighbors'])\n",
    "print('The accuracy: %.4f\\n' % gs_knn.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "It can be seen that GridSearchCV can help us to the automated hyperparameter tuning. Actually, it also store the intermediate results during the search procrss. The attribute 'cv_results_' of GridSearchCV contains much such informaiton. For example, this attribute contains the 'mean_test_score' and 'std_test_score' for the cross validation. Make use of this information to produce a plot similar to what we did in the manual way. Please check if the two plots comply with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gs_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sw/wjwplwf51d53rn6bd73ct86w0000gq/T/ipykernel_86166/1550166899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# automated hyperparameter tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv_scores_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_knn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcv_scores_stds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_knn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot the relationship\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gs_knn' is not defined"
     ]
    }
   ],
   "source": [
    "# automated hyperparameter tuning\n",
    "cv_scores_means = gs_knn.cv_results_['mean_test_score']\n",
    "cv_scores_stds = gs_knn.cv_results_['std_test_score']\n",
    "\n",
    "# Plot the relationship\n",
    "plt.errorbar(k_range, cv_scores_means, yerr=cv_scores_stds, marker='o', label='Accuracy')\n",
    "plt.ylim([0.1, 1.1])\n",
    "plt.xlabel('$K$')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "ply.slow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes Classifier\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes' theorem with the \"naive\" assumption of conditional independence between every pair of features given the value of the class variable. Bayes'theorem states the following relationship, given class variable $y$ and dependent feature vector $x_1$ through $x_n$,:\n",
    "\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "\n",
    "Using the naive conditional independence assumption, we have\n",
    "\n",
    "$$\\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align}$$\n",
    "\n",
    "Then, we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$; the former is then the relative frequency of class $y$ in the training set.\n",
    "\n",
    "*References*:\n",
    "H. Zhang (2004). The optimality of Naive Bayes. Proc. FLAIRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Gaussian Naive Bayes\n",
    "\n",
    "[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) implements the Gaussian Naive Bayes algorithm for classification on the data sets where features are continuous.   \n",
    "The likelihood of the features is assumed to be Gaussian:\n",
    "\n",
    "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$\n",
    "\n",
    "The parameters $\\sigma_y$ and $\\mu_y$  are estimated using maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo**: In this demo, we show how to build a Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sw/wjwplwf51d53rn6bd73ct86w0000gq/T/ipykernel_86166/574677687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Data split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Visualize the generated data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate a synthetica 2D dataset\n",
    "X, y = make_classification(n_samples=50, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_classes=3, n_clusters_per_class=1, \n",
    "                           weights=None, flip_y=0.01, class_sep=0.5, hypercube=True,\n",
    "                           shift=0.0, scale=1.0, shuffle=True, random_state=42)\n",
    "\n",
    "# Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "# Visualize the generated data\n",
    "colors = ['blue', 'yellow', 'green']\n",
    "for i, color in enumerate(colors):\n",
    "    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], c=color)\n",
    "plt.scatter(X_test[:, 0], X_test[:,1], c='red', marker='x', label='Testing Data')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and training a Gaussian Naive Bayes classifier model\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to predict testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Testing accuracy is: %.4f\\n' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the learned probability (model parameters)\n",
    "print('Estimated probability of classess: \\n', clf.class_prior_)\n",
    "print('Estimated mean for each Gaussian distribution: \\n', clf.theta_)\n",
    "print('Estimated variance for each Gaussian distribution: \\n', clf.sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for Class 0 and the first feature, we can have the following Gaussian disribution\n",
    "$$P(x_0 \\mid Class=0) = \\frac{1}{\\sqrt{2\\pi\\cdot0.2323}} \\exp\\left(-\\frac{(x_0 - 0.6090)^2}{2\\cdot0.2323}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 10-fold cross validation to show a more robust prediction accuracy\n",
    "clf = GaussianNB()\n",
    "scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "print('Gaussian Naive Bayes accuracy range: [%.4f, %.4f]; mean: %.4f; std: %.4f\\n' % (scores.min(), scores.max(), scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "Given the training data generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "print(X)\n",
    "#print(X.shape)\n",
    "print(y)\n",
    "\n",
    "# Firstly, let's do the parameter estimation manually without using the model\n",
    "# dot not worry about python syntax y==1 as row index and related all data\n",
    "# [:,0] first dimension all data\n",
    "X_0_C_1=X[y==1][:,0]\n",
    "print(X_0_C_1)\n",
    "\n",
    "X_1_C_1=X[y==1][:,1]\n",
    "print(X_1_C_1)\n",
    "\n",
    "X_0_C_2=X[y==2][:,0]\n",
    "print(X_0_C_2)\n",
    "\n",
    "X_1_C_2=X[y==2][:,1]\n",
    "print(X_1_C_2)\n",
    "\n",
    "# mean\n",
    "manual_means = np.array([[X_0_C_1.mean(), X_1_C_1.mean()], [X_0_C_2.mean(), X_1_C_2.mean()]])\n",
    "\n",
    "# precision=4 means 0.0000\n",
    "np.set_printoptions(precision=4)\n",
    "print('Means estaimated manually: \\n', manual_means)\n",
    "\n",
    "# variance\n",
    "manual_vars = np.array([[X_0_C_1.var(), X_1_C_1.var()], [X_0_C_2.var(), X_1_C_2.var()]])\n",
    "print('Variances estaimated manually: \\n', manual_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train a GaussianNB model and print out the learned model parameters (parameters of probability distributions). And check if the learned parameters comply with the manually estimated ones as shown above. Predict the label of a data [-0.8,-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and training a Gaussian Naive Bayes classifier model\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('Estimated means: \\n', clf.theta_)\n",
    "print('Estimated variance: \\n', clf.sigma_)\n",
    "\n",
    "# Prediction\n",
    "# For example, we created the X_new data\n",
    "X_new = [[-0.8,-1]]\n",
    "y_pred = clf.predict(X_new)\n",
    "print('Class label is: %.4f\\n' % y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Multinomial Naive Bayes\n",
    "The [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) classification model is suitable for classification with discrete features. To let the model handle to categorical data, we often need to transform the categorical values to numberic ones, through [encoding](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weather data\n",
    "weather_data = pd.read_csv('weather.csv')\n",
    "print(weather_data.head())\n",
    "print('\\nData shape: ', weather_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing and preparation\n",
    "# Firstly, we need to encode categorical values\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "coded_data = enc.fit_transform(weather_data)\n",
    "\n",
    "X = coded_data[:, 0:-1]\n",
    "y = coded_data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat and train a model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = clf_mnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is: %.4f\\n' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sw/wjwplwf51d53rn6bd73ct86w0000gq/T/ipykernel_86166/2241714463.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Estimated probability of classess: \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclf_mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_log_prior_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Estimated class-conditional probabilities for each feature: \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclf_mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Explore the learned model parameters (probabilities)\n",
    "# Note that the probabilities are in the logorithmic form. Why? The log-sum-exp trick for underflow of probability products\n",
    "\n",
    "\n",
    "print('Estimated probability of classess: \\n', np.e**clf_mnb.class_log_prior_)\n",
    "print('Estimated class-conditional probabilities for each feature: \\n', np.e**clf_mnb.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "Given the training data generated as follows. The number of data instances (6) is small while the demensionality of the data is relatively highly (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sw/wjwplwf51d53rn6bd73ct86w0000gq/T/ipykernel_86166/103367326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#return random int from 0 to 5 including 0. size this the matrux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#return random int from 0 to 5 including 0. size this the matrux\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "#print(X)\n",
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a MultinomialNB model, and predict the label of a data X_new = [[1,2,1,0,2,3,0,3,2,1,1,3,3,0,4,2,2,0,0,2,2,3,4,4,4,4,0,3,3,\n",
    "          1,1,1,2,3,1,3,0,2,2,0,4,2,4,3,2,0,1,1,1,2,3,0,0,3,4,3,3,4,\n",
    "          2,1,0,0,0,0,4,1,2,0,0,4,4,0,4,1,3,1,1,1,3,1,1,1,4,3,1,1,3,\n",
    "          2,0,0,0,3,4,1,1,4,3,2,3,4]]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a MultinomialNB model\n",
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(X, y)\n",
    "\n",
    "# Predict the class of the new data instance\n",
    "# X_new = np.random.randint(5, size=(1, 100))\n",
    "X_new = [[1,2,1,0,2,3,0,3,2,1,1,3,3,0,4,2,2,0,0,2,2,3,4,4,4,4,0,3,3,\n",
    "          1,1,1,2,3,1,3,0,2,2,0,4,2,4,3,2,0,1,1,1,2,3,0,0,3,4,3,3,4,\n",
    "          2,1,0,0,0,0,4,1,2,0,0,4,4,0,4,1,3,1,1,1,3,1,1,1,4,3,1,1,3,\n",
    "          2,0,0,0,3,4,1,1,4,3,2,3,4]]\n",
    "clf_mnb.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lecture, we discussed that if there is no occurence of some feature values, zero probabilities will appear. To overcome this issue, Laplace correction (smoothing) is proposed, as shown in the follow formula. In the [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) implementation, the parameter 'alpha' controls the way we apply smoothing. The default value is 'alpha=1.0'. Please create and train a model with no Laplace smoothing for the above data set. Compare the leaned model parameters (probabilities) with the case 'alpha=1', by checking if there are zero probabilities (note that due to the accuracy issue, zero might be represented as a signficantly small number by the computer).\n",
    "$$p(x_{yi}|y)=\\frac{N_{yi}+\\alpha}{N_y+{\\alpha}n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.1667 0.1667 0.1667 0.1667 0.1667 0.1667]\n",
      "Estimated class-conditional probabilities for each feature: \n",
      " [[2.4242e-02 6.0606e-13 6.0606e-13 1.2121e-02 1.8182e-02 6.0606e-13\n",
      "  1.2121e-02 1.8182e-02 6.0606e-13 6.0606e-03 2.4242e-02 6.0606e-03\n",
      "  6.0606e-03 6.0606e-03 6.0606e-13 6.0606e-03 2.4242e-02 6.0606e-03\n",
      "  2.4242e-02 1.8182e-02 6.0606e-13 1.2121e-02 6.0606e-03 6.0606e-13\n",
      "  6.0606e-13 1.8182e-02 2.4242e-02 1.8182e-02 2.4242e-02 6.0606e-03\n",
      "  6.0606e-13 6.0606e-03 1.2121e-02 6.0606e-13 1.8182e-02 1.2121e-02\n",
      "  6.0606e-03 1.8182e-02 6.0606e-13 2.4242e-02 6.0606e-13 6.0606e-03\n",
      "  6.0606e-13 1.2121e-02 1.8182e-02 1.8182e-02 6.0606e-13 6.0606e-13\n",
      "  6.0606e-03 2.4242e-02 6.0606e-13 6.0606e-13 1.8182e-02 1.8182e-02\n",
      "  6.0606e-03 6.0606e-03 2.4242e-02 1.2121e-02 6.0606e-13 2.4242e-02\n",
      "  2.4242e-02 6.0606e-13 1.8182e-02 1.2121e-02 1.8182e-02 1.8182e-02\n",
      "  1.2121e-02 1.2121e-02 6.0606e-13 6.0606e-03 6.0606e-13 6.0606e-13\n",
      "  1.8182e-02 1.2121e-02 6.0606e-13 6.0606e-13 6.0606e-13 1.2121e-02\n",
      "  1.8182e-02 6.0606e-13 1.2121e-02 2.4242e-02 6.0606e-13 1.8182e-02\n",
      "  6.0606e-13 6.0606e-03 1.8182e-02 6.0606e-03 1.2121e-02 1.2121e-02\n",
      "  6.0606e-03 6.0606e-13 1.2121e-02 6.0606e-13 6.0606e-13 2.4242e-02\n",
      "  6.0606e-03 1.2121e-02 1.2121e-02 2.4242e-02]\n",
      " [4.9261e-13 4.9261e-03 1.9704e-02 9.8522e-03 4.9261e-03 4.9261e-03\n",
      "  1.9704e-02 4.9261e-13 4.9261e-03 4.9261e-13 4.9261e-03 9.8522e-03\n",
      "  9.8522e-03 4.9261e-13 1.9704e-02 4.9261e-03 1.9704e-02 9.8522e-03\n",
      "  9.8522e-03 1.4778e-02 4.9261e-03 1.4778e-02 4.9261e-03 4.9261e-03\n",
      "  4.9261e-13 1.9704e-02 1.9704e-02 9.8522e-03 4.9261e-03 1.4778e-02\n",
      "  1.9704e-02 4.9261e-13 4.9261e-03 1.4778e-02 4.9261e-13 1.9704e-02\n",
      "  9.8522e-03 1.4778e-02 4.9261e-03 4.9261e-03 1.4778e-02 4.9261e-13\n",
      "  4.9261e-03 4.9261e-13 1.4778e-02 4.9261e-03 1.9704e-02 1.9704e-02\n",
      "  4.9261e-03 1.4778e-02 9.8522e-03 4.9261e-03 4.9261e-03 1.9704e-02\n",
      "  4.9261e-13 1.4778e-02 1.4778e-02 4.9261e-03 1.9704e-02 4.9261e-13\n",
      "  4.9261e-03 4.9261e-13 4.9261e-13 9.8522e-03 1.4778e-02 9.8522e-03\n",
      "  9.8522e-03 1.9704e-02 1.9704e-02 4.9261e-03 1.9704e-02 1.4778e-02\n",
      "  4.9261e-03 1.9704e-02 4.9261e-13 1.9704e-02 4.9261e-03 9.8522e-03\n",
      "  4.9261e-03 4.9261e-13 1.4778e-02 4.9261e-13 1.4778e-02 4.9261e-03\n",
      "  1.4778e-02 1.9704e-02 4.9261e-13 4.9261e-13 9.8522e-03 9.8522e-03\n",
      "  1.4778e-02 1.9704e-02 1.9704e-02 9.8522e-03 9.8522e-03 1.9704e-02\n",
      "  1.4778e-02 1.9704e-02 1.9704e-02 1.4778e-02]\n",
      " [5.1813e-13 1.0363e-02 5.1813e-03 5.1813e-13 5.1813e-03 1.5544e-02\n",
      "  5.1813e-13 1.0363e-02 5.1813e-13 2.0725e-02 5.1813e-03 5.1813e-03\n",
      "  1.5544e-02 2.0725e-02 5.1813e-13 1.0363e-02 5.1813e-13 1.0363e-02\n",
      "  1.5544e-02 1.5544e-02 2.0725e-02 5.1813e-13 1.0363e-02 5.1813e-13\n",
      "  2.0725e-02 5.1813e-13 1.5544e-02 1.0363e-02 1.5544e-02 1.0363e-02\n",
      "  5.1813e-13 1.0363e-02 2.0725e-02 5.1813e-13 5.1813e-13 5.1813e-13\n",
      "  1.0363e-02 1.0363e-02 5.1813e-13 1.5544e-02 1.5544e-02 1.0363e-02\n",
      "  5.1813e-03 5.1813e-13 2.0725e-02 1.0363e-02 1.0363e-02 1.5544e-02\n",
      "  5.1813e-03 1.0363e-02 5.1813e-03 1.5544e-02 1.0363e-02 5.1813e-03\n",
      "  5.1813e-03 1.5544e-02 1.5544e-02 5.1813e-13 2.0725e-02 1.5544e-02\n",
      "  1.0363e-02 5.1813e-03 5.1813e-13 1.5544e-02 5.1813e-03 1.0363e-02\n",
      "  1.5544e-02 5.1813e-03 1.0363e-02 5.1813e-03 2.0725e-02 1.5544e-02\n",
      "  5.1813e-13 1.0363e-02 5.1813e-03 2.0725e-02 2.0725e-02 2.0725e-02\n",
      "  5.1813e-13 5.1813e-03 1.0363e-02 2.0725e-02 2.0725e-02 1.5544e-02\n",
      "  1.0363e-02 5.1813e-03 1.5544e-02 5.1813e-03 1.5544e-02 2.0725e-02\n",
      "  1.0363e-02 1.0363e-02 2.0725e-02 1.0363e-02 2.0725e-02 5.1813e-13\n",
      "  1.5544e-02 5.1813e-03 1.0363e-02 5.1813e-03]\n",
      " [4.9261e-03 9.8522e-03 1.9704e-02 4.9261e-13 4.9261e-03 9.8522e-03\n",
      "  4.9261e-03 1.9704e-02 4.9261e-13 4.9261e-03 9.8522e-03 1.9704e-02\n",
      "  1.9704e-02 4.9261e-13 9.8522e-03 4.9261e-03 9.8522e-03 1.9704e-02\n",
      "  1.9704e-02 1.9704e-02 1.9704e-02 1.4778e-02 9.8522e-03 9.8522e-03\n",
      "  4.9261e-03 4.9261e-13 1.9704e-02 1.4778e-02 1.9704e-02 9.8522e-03\n",
      "  4.9261e-13 4.9261e-03 1.4778e-02 4.9261e-03 1.9704e-02 4.9261e-13\n",
      "  1.4778e-02 4.9261e-13 4.9261e-13 1.4778e-02 1.9704e-02 1.4778e-02\n",
      "  4.9261e-03 4.9261e-03 4.9261e-13 4.9261e-13 9.8522e-03 1.9704e-02\n",
      "  4.9261e-03 4.9261e-03 4.9261e-03 4.9261e-13 1.4778e-02 4.9261e-03\n",
      "  9.8522e-03 4.9261e-13 1.4778e-02 4.9261e-03 1.4778e-02 9.8522e-03\n",
      "  9.8522e-03 1.9704e-02 4.9261e-13 1.9704e-02 9.8522e-03 9.8522e-03\n",
      "  4.9261e-03 4.9261e-03 1.9704e-02 9.8522e-03 1.9704e-02 1.4778e-02\n",
      "  4.9261e-13 4.9261e-03 1.4778e-02 9.8522e-03 1.4778e-02 4.9261e-13\n",
      "  4.9261e-03 4.9261e-13 1.4778e-02 4.9261e-03 9.8522e-03 1.9704e-02\n",
      "  9.8522e-03 9.8522e-03 1.4778e-02 1.9704e-02 1.9704e-02 1.4778e-02\n",
      "  4.9261e-03 1.9704e-02 9.8522e-03 9.8522e-03 9.8522e-03 4.9261e-13\n",
      "  4.9261e-03 1.4778e-02 4.9261e-03 1.9704e-02]\n",
      " [5.3191e-03 1.0638e-02 5.3191e-13 1.0638e-02 1.0638e-02 1.5957e-02\n",
      "  5.3191e-03 5.3191e-03 5.3191e-03 1.5957e-02 5.3191e-03 1.5957e-02\n",
      "  5.3191e-03 2.1277e-02 1.5957e-02 5.3191e-13 1.5957e-02 5.3191e-03\n",
      "  2.1277e-02 5.3191e-13 1.5957e-02 5.3191e-03 1.0638e-02 1.0638e-02\n",
      "  2.1277e-02 5.3191e-03 5.3191e-03 1.5957e-02 1.5957e-02 2.1277e-02\n",
      "  5.3191e-13 1.0638e-02 5.3191e-03 5.3191e-13 1.0638e-02 1.5957e-02\n",
      "  5.3191e-03 1.0638e-02 5.3191e-03 5.3191e-03 5.3191e-13 2.1277e-02\n",
      "  2.1277e-02 1.0638e-02 5.3191e-13 5.3191e-03 5.3191e-13 1.0638e-02\n",
      "  1.0638e-02 1.5957e-02 2.1277e-02 5.3191e-13 1.5957e-02 2.1277e-02\n",
      "  5.3191e-03 5.3191e-13 5.3191e-03 1.5957e-02 1.5957e-02 1.0638e-02\n",
      "  5.3191e-13 1.0638e-02 1.5957e-02 5.3191e-03 2.1277e-02 2.1277e-02\n",
      "  1.5957e-02 5.3191e-13 5.3191e-13 2.1277e-02 2.1277e-02 2.1277e-02\n",
      "  1.0638e-02 5.3191e-13 5.3191e-03 1.5957e-02 5.3191e-13 2.1277e-02\n",
      "  5.3191e-13 5.3191e-03 2.1277e-02 5.3191e-03 2.1277e-02 5.3191e-13\n",
      "  2.1277e-02 2.1277e-02 1.5957e-02 1.5957e-02 5.3191e-13 5.3191e-03\n",
      "  5.3191e-03 5.3191e-13 5.3191e-03 1.5957e-02 1.5957e-02 5.3191e-13\n",
      "  1.0638e-02 5.3191e-13 5.3191e-13 1.0638e-02]\n",
      " [1.6575e-02 1.6575e-02 1.1050e-02 5.5249e-13 2.2099e-02 5.5249e-03\n",
      "  5.5249e-13 5.5249e-03 1.1050e-02 1.1050e-02 5.5249e-13 1.1050e-02\n",
      "  5.5249e-13 1.1050e-02 2.2099e-02 1.6575e-02 5.5249e-03 5.5249e-13\n",
      "  2.2099e-02 1.6575e-02 5.5249e-03 1.1050e-02 1.6575e-02 5.5249e-03\n",
      "  2.2099e-02 5.5249e-03 5.5249e-13 1.1050e-02 2.2099e-02 1.1050e-02\n",
      "  1.6575e-02 1.6575e-02 5.5249e-03 5.5249e-13 2.2099e-02 1.1050e-02\n",
      "  2.2099e-02 5.5249e-13 2.2099e-02 1.1050e-02 5.5249e-03 5.5249e-13\n",
      "  1.6575e-02 5.5249e-03 5.5249e-13 2.2099e-02 1.1050e-02 5.5249e-13\n",
      "  5.5249e-13 5.5249e-13 5.5249e-03 5.5249e-13 1.6575e-02 1.1050e-02\n",
      "  5.5249e-03 5.5249e-03 2.2099e-02 1.1050e-02 5.5249e-03 1.6575e-02\n",
      "  1.6575e-02 2.2099e-02 5.5249e-13 2.2099e-02 1.1050e-02 1.6575e-02\n",
      "  5.5249e-03 1.1050e-02 5.5249e-03 5.5249e-13 1.6575e-02 2.2099e-02\n",
      "  5.5249e-13 5.5249e-03 5.5249e-03 1.1050e-02 1.1050e-02 5.5249e-03\n",
      "  1.1050e-02 1.6575e-02 1.1050e-02 1.6575e-02 5.5249e-13 5.5249e-13\n",
      "  1.6575e-02 5.5249e-03 1.1050e-02 2.2099e-02 1.1050e-02 1.6575e-02\n",
      "  5.5249e-03 5.5249e-13 5.5249e-13 5.5249e-13 5.5249e-03 1.1050e-02\n",
      "  5.5249e-03 2.2099e-02 1.1050e-02 1.1050e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Create and train a MultinomialNB model with no Laplace smoothing\n",
    "clf_mnb = MultinomialNB(alpha=0)\n",
    "clf_mnb.fit(X, y)\n",
    "\n",
    "print('Estimated probability of classess: \\n', np.e**clf_mnb.class_log_prior_)\n",
    "print('Estimated class-conditional probabilities for each feature: \\n', np.e**clf_mnb.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Comparasion on Iris data\n",
    "## Task 6\n",
    "Compare the prediction accuaracy between KNN clasifier (use the optimal K you've identied) and Gaussian Naive Bayes. Use 10-cross validation to report the accuracy mean and standard deviation (Note this is to ensure the comparison is based on robust performace). Which classifidation mdoel is more accurate on Iris data set? Use t-test to show if the difference is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes accuracy range: [0.8667, 1.0000]; mean: 0.9533; std: 0.0427\n",
      "\n",
      "KNN Classifier accuracy range: [0.9333, 1.0000]; mean: 0.9800; std: 0.0306\n",
      "\n",
      "t, p: -1.5240, 0.1449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_gnb = GaussianNB()\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=13)\n",
    "\n",
    "scores = cross_val_score(clf_gnb, iris_data['data'], iris_data['target'], scoring='accuracy', cv=10)\n",
    "print('Gaussian Naive Bayes accuracy range: [%.4f, %.4f]; mean: %.4f; std: %.4f\\n'\n",
    "      % (scores.min(), scores.max(), scores.mean(), scores.std()))\n",
    "scores_gnb = scores\n",
    "\n",
    "scores = cross_val_score(clf_knn, iris_data['data'], iris_data['target'], scoring='accuracy', cv=10)\n",
    "print('KNN Classifier accuracy range: [%.4f, %.4f]; mean: %.4f; std: %.4f\\n'\n",
    "      % (scores.min(), scores.max(), scores.mean(), scores.std()))\n",
    "scores_knn = scores\n",
    "\n",
    "# This is to show t-test on their performances.\n",
    "from scipy.stats import ttest_ind\n",
    "t, p = ttest_ind(scores_gnb, scores_knn)\n",
    "print ('t, p: %.4f, %.4f\\n' % (t, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
